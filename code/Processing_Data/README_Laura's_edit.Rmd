---
title: "README"
author: "Ben Cortes"
date: "November 16, 2018"
output: html_document
---

```{r}
library(tidyverse)
```

Transpose function that works on tables with headers. First imports table with headers as part of the table, then transposes, then removes the top row and makes it the new headers. Transpose SNP source file (SNPs as rows)

```{r}

# transpose function for files with headers, second argument allows specifying what reading function to use (read_csv/tsv/etc)

transpose_headed_file <- function(fname, read_type) {                      
  
  # set fun as second argument (which is a reading function). Use fun to read in the first argument (file) ignoring the first row (headers)
  
  fun = read_type 
  tempfile <- fun((paste(fname)), col_names = F)            
  
  # transpose tempfile, then coerce it into a tibble. Setfirst row as column headers, then delete first row
  
  new_tib <- as.tibble(t(tempfile))                             
  names(new_tib) <- as.character(unlist(new_tib[1,]))            
  new_tib <- new_tib[-1,]                                        
  return(new_tib)                                                   
}

tran_tomatosnps <- transpose_headed_file("../../data/Suppl_Table_2.csv", read_csv)       

```

```{r}
# Laura's version--transpose--
# NOTE FOR BEN--this result is just like yours, so you can keep your work here if you want :)
full.geno <- read_csv("../../data/Suppl_Table_2.csv")
tran_tomatosnps.L <- as_tibble(cbind(SNPs=colnames(full.geno), t(full.geno))) # transpose full.geno and cbind the SNP names to it
colnames(tran_tomatosnps.L) <- tran_tomatosnps.L[1,] # add the column names since they were moved to row 1 by t()
tran_tomatosnps.L <- tran_tomatosnps.L[-1,] # remove row 1 now that column names are where they should be
head(tran_tomatosnps.L) # visually inspect transposed geno file
```

SNP source file lacked a header for column 1, so gave it one for downstream purposes. Mutate to add a new column called NA_count. Sum up the number of NAs per row, and set that value as NA_count for that row

```{r}

colnames(tran_tomatosnps)[1] <- "Sample" 
tran_tomatosnps <- mutate(tran_tomatosnps, NA_count = rowSums(is.na(tran_tomatosnps)))
write_tsv(tran_tomatosnps, "output_tests/tran_tomatosnps.tsv")

```

Remove SNPs with greater than 10% missing data (more than 10% of row is NA). Authors removed 240, reducing the original total from 7720 to 7480. For some reason the following code reduces the original SNP count down to 7469.

```{r}

tomato_snps_no_missing <- tran_tomatosnps %>%
  filter(NA_count <= (0.1 * (ncol(tran_tomatosnps) - 2)))
write_tsv(tomato_snps_no_missing, "output_tests/tomatosnps_nomissing.tsv")

```

```{r}
# Laura's version: keep only snps with less than (or equal to) 10% missing data

tomato_snps_no_missing.L <- tran_tomatosnps.L %>%
  mutate(num.missing=rowSums(is.na(.))) %>%
  filter(num.missing <= (.1*(ncol(tran_tomatosnps.L)-1)))
# sanity check:
stopifnot(tomato_snps_no_missing.L$num.missing <= (.1*(ncol(tran_tomatosnps.L)-1)))
# remove missing data count column:
tomato_snps_no_missing.L <- tomato_snps_no_missing.L %>%
  select(-num.missing)

# check if this is the same as Ben's:
stopifnot(all.equal(tomato_snps_no_missing$Sample, tomato_snps_no_missing.L$X1))
# NOTE FOR BEN--this result is just like yours, so you can keep your work here too :)

```

Remove SNPs where major allele frequency >0.95. Authors removed 1137 SNPs. 

```{r}

# First, unite contents of all rows except for the Sample column into a string

good_allele_freq <- tomato_snps_no_missing %>%
  unite("string_format", -"Sample", sep = "")

# Next, make a new column with the row strings, but with all "NA"s removed. Unite will take NA values and insert them into the string as "NA." Create columns to count the total alleles (and to check for any leftover "N"s from "NA"s). Remove original string column. 

good_allele_freq <- mutate(good_allele_freq, true_string = str_replace_all(string_format, "NA", "")) %>%
  mutate(a_count = str_count(true_string, "A"),
                 g_count = str_count(true_string, "G"),
                 c_count = str_count(true_string, "C"),
                 t_count = str_count(true_string, "T"),
                 n_count = str_count(true_string, "N"),
                 string_format = NULL)

# Finally, filter out any row that has a base_count value higher than 95% of the lenght of the true_string string for that row. The following code reduces the total SNPs to 6477, but the authors' code reduced it to 6343 (134 less).

good_allele_freq <- filter(good_allele_freq, a_count <= (0.95 * (str_length(true_string)))) %>%
              filter(g_count <= (0.95 * (str_length(true_string)))) %>%
              filter(c_count <= (0.95 * (str_length(true_string)))) %>% 
              filter(t_count <= (0.95 * (str_length(true_string)))) 

# Now join the SNPs that remain after removing the SNPs with higher than 95% allele frequency to the tibble of SNPs that have less than 10% missing data

tomato_snps_first_cull <- left_join(good_allele_freq, tomato_snps_no_missing, by = c("Sample" = "Sample"))
tomato_snps_first_cull <- tomato_snps_first_cull[, -c(2:7,1016)]
write_tsv(tomato_snps_first_cull, "output_tests/tomatosnps_first_cull.tsv")

```

```{r}
# Laura's version: remove SNPs with major allele frequency >0.95, aka minor allele frequency <= 0.05
##NOTE: working with the dataset with missing data already removed, different from Ben ###

###NOTE FOR BEN: there's no need to join the two datasets, and by doing so you're introducing errors. INSTEAD, just need to do the removal of the maf > 0.95 as a second step on the data from which you already removed missing data

# find major allele (aka reference allele here) for each SNP
num.geno <- tomato_snps_no_missing.L %>%
  unite(united, -1, sep="", remove=F) %>%
  mutate(united_noNA=str_replace_all(united, "NA", ""), # remove NA values
         count.A=str_count(united_noNA, "A"), # count each nucleotide in the SNPs
         count.C=str_count(united_noNA,"C"),
         count.G=str_count(united_noNA, "G"),
         count.T=str_count(united_noNA,"T"),
         max.count=pmax(count.A, count.C, count.G, count.T), # find the max count of any NT for a given SNP
         ref.allele=ifelse(count.A==max.count, "A", # set the reference allele
                           ifelse(count.C==max.count, "C",
                                  ifelse(count.G==max.count, "G", "T")))
  ) 

# find major allele frequency
num.geno <- num.geno %>%
  mutate(major.af=max.count/str_length(united_noNA)) # major allele frequency is the count of the major allele (max.count) / total # of alleles known for that SNP (the length of the string united_noNA)

# sanity check--major allele frequency must be in the interval [0.5,1]
stopifnot(num.geno$major.af <=1)
stopifnot(num.geno$major.af >= 0.5)
plyr::count(is.na(num.geno$ref.allele))

# checking something--
num.geno %>%
  filter(major.af==0.5)
####NOTE--at this point there are 5 snps with maf =0.5, and therefore we don't know what the reference allele here is! Need to see what to do if they remain in the dataset...

# remove snps with maf > 0.95 (that is, keep those with maf <= 0.95); remove unneeded columns and rename others
tomato_snps_first_cull.L <- num.geno %>%
  filter(major.af<=0.95) %>%
  select(-united_noNA, -united, -count.A, -count.C, -count.G, -count.T, -max.count, -ref.allele, -major.af) %>%
  rename(marker=X1)

```

Remove SNPs that are separated by less than 0.1 centi-Morgans of genetic distance. According to the rest of my group (who works with plants and plant genomics), when studies remove SNPs based on genetic distance they remove only one of the two SNPs that fall below the distance cutoff. The authors do not state how they chose which SNP to remove (whether they chose the SNP with the lower distance, the higher distance, or if they chose randomly). According to the paper, "genetic distances were based on the genetic maps of Sim et al." (https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0040563)." However, that reference has eight different genetic maps, and the authors don't state which one(s) they used. After reading through the paper, Table S8 seems to be the most complete genetic map, so I'm using that one. This map was generated by Sim et al. from two preexisting maps of the tomato crosses EXPEN and EXPIM. The first rows contain SNPs found in both the EXPEN and EXPIM maps, and the later rows are those SNPS that are found in only one or the other. 

Separate Table S8 into 3 tibbles, each with the SNP column. PhysMap contains the physical locations of each SNP in Mbp. EXPEM and EXPIM contain the location of each SNP in cM; these later two have missing values for SNPS not found in their respective maps.

```{r}

# Read in the csv, then remove top two rows and give new column headers to get around the merged cells in the original csv file that disrupt the tibble. Separate into EXPEN and EXPIM files

S8_map <- as.tibble(read_csv("../../data/S8_Map_Sim_etal.csv"))
S8_map <- S8_map[-c(1,2),]
colnames(S8_map) <- c("Group", "SNP_ID", "Physmap_Chr", "Physmap_Mbp", "EXPEN_Chr", "EXPEN_cM", "EXPIM_Chr", "EXPIM_cM")

EXPEN <- S8_map[, c(2,5,6)]
EXPIM <- S8_map[, c(2,7,8)]

# EXPIM map had some chromosomes listed as "CHR#" instead of just "#". Used gsub to replace

EXPIM$"EXPIM_Chr" <- gsub("CHR0", "", EXPIM$"EXPIM_Chr")
EXPIM$"EXPIM_Chr" <- gsub("CHR", "", EXPIM$"EXPIM_Chr")

# Arrange by chromosome column, then by cM. Remove loci that aren't polymorphic in that map (NA)

EXPEN <- arrange(EXPEN, as.numeric(EXPEN_Chr), as.numeric(EXPEN_cM)) 
EXPIM <- arrange(EXPIM, as.numeric(EXPIM_Chr), as.numeric(EXPIM_cM))

EXPEN <- EXPEN[complete.cases(EXPEN),]
EXPIM <- EXPIM[complete.cases(EXPIM),]

```

Using function created for the R assignment, split rows of EXPEN and EXPIM into smaller files based on chromosome. Map joining package will need separated files. 

Files with identical values in a user-defined, ordered column will be added to same file (ie, all files with same value in a column will be placed into a file). Output to files with filenames bearing the pattern "'third input' 'value of column at given iteration' 'fourth input'". Input: dataframe, number of column, anything to be written in the output filename before the column value (including the file path), anything to be written in the output filename after the column value (including the file type).

```{r}

split_by_ordered_column <- function(fname, cnum, prefix, suffix) {
#dataframe to split = fname
#column to split by = cnum
#first part of filename = prefix
#second part of filename = suffix

for (i in 1:nrow(fname)) {
#iterate through all rows of dataframe
  
  if(i == 1 || fname[i,cnum] != fname[i-1,cnum]) {
  #if the first row, or if the column value isn't the same as the previous one
      
    write_tsv(fname[i,], paste(prefix, fname[i,cnum], suffix, sep = ""))
    #create a dataframe named with the current (ith) row named("prefix, current column
    #value, suffix")  
    
    } else {
        write_tsv(fname[i,], paste(prefix, fname[i,cnum], suffix, sep = ""), append = T)
    #otherwise, append ith row to current dataframe
      
    }
  }
}

split_by_ordered_column(EXPEN, 2, "sorted_data/CHR_", "_EXPEN")
split_by_ordered_column(EXPIM, 2, "sorted_data/CHR_", "_EXPIM")

```

```{r}
# Laura's version: create LPmerge input files
# need to have a separate input file for each map, for each chromosome--so, need to split the files by map and chromosome
# each file should be formatted to have headers "marker" and "cM" and the observations for these variables in the rows beneath

# read raw map file and tidy it
S8_map.L <- read_csv("../../data/S8_Map_Sim_etal.csv")
# set column names based on inspection of original csv:
colnames(S8_map.L) <- c("Group", "marker", "physical.chr", "physical.mbp", "EXPEN.chr", "EXPEN.cM", "EXPIM.chr", "EXPIM.cM")
# remove rows that contain column names rather than data
S8_map.L <- S8_map.L[-c(1:2),]
# correct type of columns:
S8_map.L$EXPEN.cM <- as.numeric(S8_map.L$EXPEN.cM)
S8_map.L$EXPIM.cM <- as.numeric(S8_map.L$EXPIM.cM)

# visually check:
head(S8_map.L)

# separate into EXPEN and EXPIM maps:
###NOTE TO BEN--you can use this code below to separate the files in code rather than manually in Excel; yours wasn't wrong this is just nicer ####

# check the values in the different columns to potentially use to split the file
unique(S8_map.L$Group) 
unique(S8_map.L$EXPEN.chr)
unique(S8_map.L$EXPIM.chr)
# EXPIM.chr contains not only chromosomes listed as 1,2,3,... but also as CHR01, CHR02, ...
# standardize these chromosome names
S8_map.L$EXPIM.chr <- gsub("CHR0", "",S8_map.L$EXPIM.chr)
S8_map.L$EXPIM.chr <- gsub("CHR", "", S8_map.L$EXPIM.chr)
# and check results:
unique(S8_map.L$EXPIM.chr)
# and make all chromosomes numeric:
S8_map.L$EXPIM.chr <- as.numeric(S8_map.L$EXPIM.chr)
S8_map.L$EXPEN.chr <- as.numeric(S8_map.L$EXPEN.chr)

# separate into maps for EXPEN and EXPIM, separated by chromosome
# process the EXPIM map
map.EXPIM <- filter(S8_map.L, Group %in% c("Common", "EXPIM 2012")) %>% #separate out SNPs in EXPIM
  select(-c(Group, physical.chr, physical.mbp, EXPEN.chr, EXPEN.cM)) %>% # remove unneeded columns
  rename(chr=EXPIM.chr, cM=EXPIM.cM) %>% # rename to match LPmerge format
  group_by(chr) #group by Group
split.EXPIM <- split(map.EXPIM, map.EXPIM$chr) # split dataframes by chr and save in a list
# sanity check: no missing chromosomes
stopifnot(sum(is.na(map.EXPIM$chr))==0)
# remove chromosome column
for (i in 1:length(split.EXPIM)) {
  split.EXPIM[[i]] <- split.EXPIM[[i]]%>%
    ungroup()%>%
    select(-chr)
}

# repeat for EXPEN
map.EXPEN <- filter(S8_map.L, Group %in% c("Common", "EXPEN 2012")) %>% #separate out SNPs in EXPEN
  select(-c(Group, physical.chr, physical.mbp, EXPIM.chr, EXPIM.cM)) %>% # remove unneeded columns
  rename(chr=EXPEN.chr, cM=EXPEN.cM) %>% # rename to match LPmerge format
  group_by(chr) #group by Group
split.EXPEN <- split(map.EXPEN, map.EXPEN$chr) # split dataframes by chr and save in a list
# sanity check: no missing chromosomes
stopifnot(sum(is.na(map.EXPEN$chr))==0)
# remove chromosome column
for (i in 1:length(split.EXPEN)) {
  split.EXPEN[[i]] <- split.EXPEN[[i]]%>%
    ungroup()%>%
    select(-chr)
}

```
In order to use the genetic map (Table S8) to remove SNPs that fall within 0.1 cM the SNPs common between the EXPEN and EXPIM maps that compose Table S8 to create a consensus map. The paper does not mention how this was calculated, so we used the package LPmerge (Jeffrey Endelman, "Merging Linkage Maps by Linear Programming, 2018). LPmerge creates multiple consensus maps and outputs them as dataframes in a list along with statistics for each dataframe. The user must then manually assess the quality of each dataframe via the statistics and pick the best one (the one with "the lowest mean and sd for RMSE"). Lowest mean value was prioritized.

```{r}

library(LPmerge)

map_merger <- function(file1, file2) {
# call library and create function to run LPmerge on two files
  
  file1_merge <- as.data.frame(read_tsv(file1))
  file2_merge <- as.data.frame(read_tsv(file2))
  # read in the files as dataframes (LPmerge needs dataframes)
  
  map2 <- list(file1_merge[,c(1,3)], file2_merge[,c(1,3)])
  names(map2) <- c("EXPEN", "EXPIM")
  str(map2)
  # add the SNP_ID and cM columns to a list, give the list names, and format it as a string
  
  merged_map2 <- LPmerge(map2, max.interval=1:4)
  # call LPmerge which outputs a list with four datatables
  
}

# add all chromosome files for EXPEN and EXPIM to vectors

EXPEN_list <- c("sorted_data/CHR_1_EXPEN", "sorted_data/CHR_2_EXPEN", "sorted_data/CHR_3_EXPEN", "sorted_data/CHR_4_EXPEN", "sorted_data/CHR_5_EXPEN", "sorted_data/CHR_6_EXPEN", "sorted_data/CHR_7_EXPEN", "sorted_data/CHR_8_EXPEN", "sorted_data/CHR_9_EXPEN", "sorted_data/CHR_10_EXPEN", "sorted_data/CHR_11_EXPEN", "sorted_data/CHR_12_EXPEN")

EXPIM_list <- c("sorted_data/CHR_1_EXPIM", "sorted_data/CHR_2_EXPIM", "sorted_data/CHR_3_EXPIM", "sorted_data/CHR_4_EXPIM", "sorted_data/CHR_5_EXPIM", "sorted_data/CHR_6_EXPIM", "sorted_data/CHR_7_EXPIM", "sorted_data/CHR_8_EXPIM", "sorted_data/CHR_9_EXPIM", "sorted_data/CHR_10_EXPIM", "sorted_data/CHR_11_EXPIM", "sorted_data/CHR_12_EXPIM")

# iterate through the vectors of chromosome files and run the merging function on them

merged_maps <- list()

for (i in 1:12) {
  merged_maps[[i]] <- map_merger(EXPEN_list[i], EXPIM_list[i])
}

```

``` {r}
# Laura's version: LPmerge run and create consensus map

library(LPmerge)

# Make a function to run LPMerge:
# based on Ben's function to run LPmerge, but using the df already loaded rather than writing and then reading in files:
map_merger.L <- function(EXPEN.df, EXPIM.df) {
  
  # make into data frames:
  EXPEN.df <- as.data.frame(EXPEN.df)
  EXPIM.df <- as.data.frame(EXPIM.df)
  
  # put in list, then format as string (from LPMerge tutorial at https://potatobreeding.cals.wisc.edu/wp-content/uploads/sites/161/2014/01/LPmerge_tutorial.pdf)
  Maps <- list(EXPEN.df, EXPIM.df)
  names(Maps) <- c("EXPEN", "EXPIM")
  str(Maps)

  # run LPmerge on these files, using default settings for max.interval:
  merged.map <- LPmerge(Maps, max.interval = 1:4)
  
}


# sanity check--are both input lists the same size?
stopifnot(length(split.EXPEN)==length(split.EXPIM))

### NOTE FOR BEN-- because I stored my data in lists rather than having to read it back in (see my version of dataprocessing for LPmerge, above), I don't have to make the manual EXPEN_list and EXIM_list like you did to run the loop. Again, this wasn't wrong, but I just changed it to make it easier and as a double-check

# Use the function above to run LPmerge on all chromosomes
merged_maps.L <- list()

for (i in 1:length(split.EXPEN)) {
  merged_maps.L[[i]] <- map_merger.L(split.EXPEN[[i]], split.EXPIM[[i]])
}

# manually inspect the output to determine which of the four consensus maps generated for each chromosome is the best.
# the criteria, according to the LPmerge tutorial (at https://potatobreeding.cals.wisc.edu/wp-content/uploads/sites/161/2014/01/LPmerge_tutorial.pdf): choose the lowest mean RMSE, and break ties using the lowest sd for RMSE.

# NOTE for BEN--making a vector of the best consensus maps makes it a little more code-friendly if you want to try that. Again, not wrong, just a change.

# After doing so, the results for the best consensus maps are, for chromosomes 1-12 in order:
best <- c(1,2,4,1,1,1,3,4,3,3,1,2)

# Now, make the overall consensus map:

# as a list:
consensus.map.list <- list()
for (i in 1:length(merged_maps.L)) {
  consensus.map.list[[i]] <- merged_maps.L[[i]][[best[i]]] %>%
    mutate(chr=i) %>% # add column to save chromosome number (I visually inspected to be certain that merged_maps.L[[i]] does in fact correspond to chromosome i)
    select(marker, chr, cM=consensus) # arrange (and rename) needed columns
}

# as one dataframe:
consensus.map <- as.tibble(do.call("rbind", consensus.map.list))

```

Manually sort through statistics for each of the four datafames per chromosome, and pick the one with the lowest mean (use lowest standard deviation for ties). Then merge the dataframes into one and calculate the genetic distance between each SNP. Finally, remove SNPs which are greater than 0.1 cM separated from their neighbors.

```{r}

# use rbind to bind the rows of the 12 dataframes into one. Then remove all SNPs from millimorgan_snps that aren't present in the authors' original dataset

consensus_map <- rbind(merged_maps[[1]][[1]], merged_maps[[2]][[2]], merged_maps[[3]][[4]], merged_maps[[4]][[1]], merged_maps[[5]][[1]], merged_maps[[6]][[1]], merged_maps[[7]][[2]], merged_maps[[8]][[4]], merged_maps[[9]][[3]], merged_maps[[10]][[3]], merged_maps[[11]][[1]], merged_maps[[12]][[2]])

millimorgan_snps <- consensus_map[,1:2]
millimorgan_snps <- semi_join(millimorgan_snps, tomato_snps_first_cull, by = c("marker" = "Sample"))

# Use mutate to add an additional column, distance, which result of subtracting the previous consensus value from the the consensus value for a given row. Use lag to reference the previous value 

millimorgan_snps <- mutate(millimorgan_snps, distance = consensus - lag(consensus))

# use filter to remove SNPs which are less than 0.1 cM from the previous SNP. In theory, this would be a problem if there were many consecutive SNPs separated by distances of < 0.1 cM such that the total distance spanned from the first to last SNP was actually greater than 0.1 cM (since the filter paramaters only compare two consecutive SNPs). However, a cursory look through the data turns up none of these scenarios. The second two arguments account for the first SNP in chromosome 1 (the first SNP in the dataframe) and the first SNP in the next 11 chromosomes. Authors state that their final number of SNPS was 2313, but the result of this code yields a remaining 1461 SNPs.

millimorgan_snps <- filter(millimorgan_snps, (distance > 0.1 | is.na(distance) | distance < 0))
write_tsv(millimorgan_snps, "output_tests/millimorgansnps.tsv")

```

``` {r}
# Laura's version: use consensus map to thin our dataset

### NOTE FOR BEN-- there's a problem here. We need to address it. See below for how I did. ####

# PROBLEM: as seen below, not all SNPs in my data are even in the consensus map. In fact, there are SNPs that are in only the consensus map AND SNPs that are in only the Blanca paper data.
# Checking the original paper from which the consensus maps came, we are in fact using the most complete map; these other SNPs that are in our dataset simply are not mapped. 
# The text of the methods of the Blanca, et al. paper said that "SNPs that mapped closer than 0.1 cM were removed". So, I will proceed under the assumption that the SNPs that ARE found in the map were filtered following this criteria, but that then the SNPs that were not in the map were simply retained, because they are not known to map closer than 0.1 cM.

map.markers <- S8_map.L$marker
my.markers <- tomato_snps_first_cull.L$marker

shared <- map.markers[map.markers %in% my.markers]

map.only <- map.markers[!(map.markers %in% my.markers)]
my.only <- my.markers[!(my.markers %in% map.markers)]
# there are 4206 shared snps + 1090 only in map = 5296 in map. OK.
# there are 4206 shared snps + 2265 only in mine = 6471 in my markers. OK.

# Now, why are these different?

# several in the map seem to have prefixes before the "solcap_..." that are not present in my data. Remove the prefixes:
map.markers <- gsub("^.*?solcap", "solcap", map.markers)
shared <- map.markers[map.markers %in% my.markers]

map.only <- map.markers[!(map.markers %in% my.markers)]
my.only <- my.markers[!(my.markers %in% map.markers)]
# this increased the total to 4422 shared entries out of 5296 in the map

# there are now quite a few snps with names formatted as "solcap_snp_sl_..." that should match those in my data, if they are present, but they are not. So, remove these to focus on other remaining problems:
map.only <- gsub("^.*solcap_snp_sl_.*", "REMOVE", map.only)
map.only <- map.only[map.only!="REMOVE"]


# After visual inspection, NONE of the remaining SNPs show any obvious way of connecting the entries from the two lists to one another. So, will have to proceed knowing that only some of the SNPs in the Blanca, et al. dataset are actually represented in the map.
# make a new, subset consensus map containing ONLY those SNPs that are shared with the Blanca paper:
consensus.map$marker <- gsub("^.*?solcap", "solcap", consensus.map$marker) 
new.consensus.map <- consensus.map[consensus.map$marker %in% shared,]

# Now, thin these based on the 0.1 cM criteria:
# The paper did not say how they decided which SNP out of a group of nearby SNPs to remove, so we will arbitrarily choose to keep the first SNP found and then exclude all SNPs found within the next 0.1 cM, then repeat the process from the next SNP that is outside that region. 

# find distance between SNPs and keep if they are >=0.1 cM from one another
consensus.map.thinned <- new.consensus.map %>%
  mutate(distance.lag = cM-lag(cM), # find distance between adjacent SNPs
         KEEP=ifelse(chr != lag(chr),TRUE,ifelse(distance.lag >= 0.1,TRUE,FALSE))) # mark to keep SNPs that are separated by at least 0.1 cM, as well as those on the end of the chromosome

# check that this worked before filtering:
consensus.map.thinned %>% filter(is.na(KEEP)) # the only entry with NA is the first one, which is ok; keep it

# It is potentially possible that there may be many SNPs in a row that are each within 0.1 cM of one another and so none of them will be kept, but that span a total distance of over 0.1 cM and therefore at least some of them should be kept. For example, if there were SNPs on chr 1 at 0, 0.05, 0.12, and 0.3 cM, the algorithm above would keep only 0 and 0.3 when in reality we want to keep 0, 0.12, and 0.3 so that each SNP that is kept is separated by more than or equal to 0.1 cM. These cases will only be found among SNPs that were marked KEEP==FALSE and which are separated from the nearest SNP that is also marked KEEP==FALSE by less than 0.1 cM (as explained above), and more than 0 cM (otherwise they are identical and only one should in fact be kept).  Below, use the column "inspect" to indicate a member of a potential group of SNPs that could be causing this problem. 
check <- consensus.map.thinned %>%
  filter(KEEP == F) %>%
  filter(distance.lag>0) %>%
  mutate(new.distance.lag = cM-lag(cM),
         inspect= ifelse((new.distance.lag<=0.1 & new.distance.lag >0),"YES","-")) 
sum(check$inspect=="YES", na.rm=T)
# There are only 11 cases where this problem could potentially occur. Although a larger data set could make this solution intractable, for this small dataset I will manually inspect the data to find if any of these SNPs should in fact be included.

# After manual inspection, this situation did occur 3 times. To solve, need to manually include the following SNPs:
# solcap_snp_sl_25987
# solcap_snp_sl_25442
# solcap_snp_sl_19031
# also want to manually keep the first SNP (currently, KEEP==NA for that snp): solcap_snp_sl_15058
consensus.map.thinned$KEEP[consensus.map.thinned$marker=="solcap_snp_sl_25987"] <- TRUE
consensus.map.thinned$KEEP[consensus.map.thinned$marker=="solcap_snp_sl_25442"] <- TRUE
consensus.map.thinned$KEEP[consensus.map.thinned$marker=="solcap_snp_sl_19031"] <- TRUE
consensus.map.thinned$KEEP[consensus.map.thinned$marker=="solcap_snp_sl_15058"] <- TRUE

# Now, can pull out the list of SNPs to remove:
remove.snps <- consensus.map.thinned$marker[consensus.map.thinned$KEEP==F]

# and remove these snps from the dataset created above after MAF and missing data processing:
final.snps <- tomato_snps_first_cull.L %>%
  filter(!(marker %in% remove.snps))

```

Create the final SNP file. Join the list of SNPs that are separated by at least 0.1 cM to the list of SNPs that have less than 10% missing data and an allele frequency of less than 95%.  Write output to a tsv.

### NOTE for BEN-- that won't work. Again, you can't weed out what things are separated by less than 0.1 cM and THEN eliminate more (by joining with those files with fewer SNPs). This is what we talked about--you remove too many this way, and even if by chance you didn't, the removal based on cM is listed as the last step here and so it NEEDS to be the last step--if not, the results will not be the same. So. You need to work ONLY on the snps that you kept above.
### NOTE for BEN--another problem. The snps in the consensus map DO NOT match all the snps in our files, so there's no way for us to join them nicely. See my code. 

```{r}
final_SNPs <- semi_join(tomato_snps_first_cull, millimorgan_snps, by = c("Sample" = "marker"))

write_tsv(final_SNPs, "final_snps.tsv")
write_tsv(final_SNPs, "output_tests/final_snps.tsv")

```



Go through and remove duplicate accessions (lines of tomatoes that were genotyped twice) from the text file 12864_2015_1444_MOESM1_ESM containing the passport data. The names of the accessions in this file differ from their names in the raw SNP datafile, so Laura's code was used to change the names in the passport file to those of the SNP file.

```{r}
# Laura's version: process data before removing duplicate accessions

# transpose final.snps so that it matches the original layout of the genotype file
tran.final.snps <- as_tibble(cbind(Sample=colnames(final.snps), t(final.snps))) # transpose final.snps and cbind the accession names to it
colnames(tran.final.snps) <- c("Sample", tran.final.snps[1,-1]) # add the column names since they were moved to row 1 by t()
tran.final.snps <- tran.final.snps[-1,] # remove row 1 now that column names are where they should be
head(tran.final.snps) # visually inspect transposed geno file

# import accessions' passport information, and edit names to match those in the snps file (following code found in `2_Eigensoft_geno_format.Rmd`)
passport.data <- read_tsv("../../data/12864_2015_1444_MOESM1_ESM.txt")
# After manual inspection, some of the problems seem to be: 
# different cases (upper vs lower),
# periods vs commas, 
# and spaces vs underscores
# So, manually edit passport data so that it will match SNP data:
passport.data$Sample <- gsub("\\,", "\\.", passport.data$Sample ) # switch commas to periods
passport.data$Sample  <- gsub("PAS16398", "pas16398", passport.data$Sample ) # change case
passport.data$Sample  <- gsub("PAS16401", "pas16401", passport.data$Sample ) # change case
passport.data$Sample  <- gsub("PI 379051", "PI_379051", passport.data$Sample ) # change space to underscore
# check that this fixed problem and all entries match up:
stopifnot(sum(!(passport.data$Sample %in% tran.final.snps$Sample))==0)
stopifnot(sum(!(tran.final.snps$Sample %in% passport.data$Sample))==0)


```


```{r}

# transpose the final_SNPs file (which has all pertinent SNPs removed). Make the column name for the new first column, the accession column, "Accession." Arrange by the Accession column

tran_final_snps <- as.tibble(transpose_headed_file("final_snps.tsv", read_tsv))
colnames(tran_final_snps)[1] <- "Accession"
tran_final_snps <- arrange(tran_final_snps, as.character(Accession))
write_tsv(tran_final_snps, "output_tests/tran_final_snps.tsv")

# import passport data and begin changing accession names

passport_data <- as.tibble(read_tsv("../../data/12864_2015_1444_MOESM1_ESM.txt"))

# start Laura's code, variable names modified:
# After manual inspection, some of the problems seem to be: 
# different cases (upper vs lower),
# periods vs commas, 
# and spaces vs underscores
# So, manually edit passport data so that it will match SNP data:

passport_data$Sample <- gsub("\\,", "\\.", passport_data$Sample) # switch commas to periods

# Now only 3 are not joined, so manually edit these to match:
passport_data$Sample <- gsub("PAS16398", "pas16398", passport_data$Sample) # change case
passport_data$Sample <- gsub("PAS16401", "pas16401", passport_data$Sample) # change case
passport_data$Sample <- gsub("PI 379051", "PI_379051", passport_data$Sample) # change space to underscore

write_tsv(passport_data, "passport_data.tsv")

```

According to the authors: 

"When an accession was genotyped more than once and both genotypes were inconsistent (e.g., both samples were classified in different subgroups in the PCA) all data for the accession was removed from the analysis (see Additional file 1: Table S1), unless it was clear based on the passport information, which genotype was correct (e.g., two entries from the same SLC accession collected in Peru, one grouping with other Peruvian accessions and another grouping with the mixture group). In total 8 genotypes out of the 1,008 were removed due to inconsistent data...unless stated to the contrary, only one randomly chosen genotype representative of the uniquely named accessions was used."

What exactly "unless it was clear based on the passport information" means could vary from the provided example depending on the situation, so was not amenable to coding. We decided that the best thing to do would be to go through and inspect the duplicate data manually after arranging it. The following workflow was used in Excel.

1. The file used (`duplicate_accessions.csv`) was generated in R (below) by filtering for duplicated accessions in the passport.data tibble.
2. The accession duplicates were checked for inconsistencies in the columns that contain information from the PCA; that is, the species, group1, and group2 columns.
3. If there were no inconsistencies found, one sample was chosen at random (using a random number generator) for that accession to keep. The other samples for the accession were marked to delete.
4. If inconsistencies were found, the passport and geographic data columns were assessed to determine which accession was "correct." We took "conflicting classification" to indicate that a given sample was not the correct sample for that accession. However, if it was not clear which sample was correct, we removed the accession entirely.
5. We saved the file with the lines to delete and keep marked as `duplicate_accessions_new.csv`.

```{r}
# Laura's version: export passport file to use to inspect duplicates in Excel

# find duplicate accessions in the passport file:
dups <- passport.data$Accession[duplicated(passport.data$Accession)] 
u.dups <- unique(dups)
passport.data.dup <- passport.data %>%
  filter(Accession %in% u.dups) %>%
  arrange(Accession)
write_csv(passport.data.dup, "duplicate_accessions.csv")

# after the above workflow in Excel, read the data in:
new.passport.data.dup <- read_csv("duplicate_accessions_new.csv") 

remove <- filter(new.passport.data.dup, `keep? (1 for yes, 0 for no)`==0) 

# remove the samples to be removed from the dataset tran.final.snps:
final.data <- filter(tran.final.snps, !(Sample %in% remove$Sample))

# write the final dataset:
write_csv(final.data, "../../data/final_data.csv")
```


```{r}

# import list of accessions that are to be removed. Antijoin the list of accessions to be removed with the transposed final SNPs file, resulting in a tibble where all pertinent SNPs have been removed and all the appropriate, duplicated accessions have been removed.

remove_accessions <- as.tibble(read_tsv("removed_accessions.tsv"))
final_data <- anti_join(tran_final_snps, remove_accessions, by = c("Accession" = "Sample"))
write_csv(final_data, "../../data/final_data.csv")

```

```{r}
# Laura's version: data for the rarefaction and LD analysis
# According to the methods in Blanca et al, this data was NOT filtered with the 0.1 cM requirement; otherwise, it was processed as the other data was above.

# so, this means I need to take the dataset that had MAF and missing data filtering done (`tomato_snps_first_cull.L`), transpose it, and remove the duplicate accessions as above:

# transpose `tomato_snps_first_cull.L`
LD.data <- as_tibble(cbind(Sample=colnames(tomato_snps_first_cull.L), t(tomato_snps_first_cull.L))) # transpose tomato_snps_first_cull.L and cbind the accession names to it
colnames(LD.data) <- c("Sample", LD.data[1,-1]) # add the column names since they were moved to row 1 by t()
LD.data <- LD.data[-1,] # remove row 1 now that column names are where they should be
head(LD.data) # visually inspect transposed geno file

# remove duplicates
final.LD.data <- filter(LD.data, !(Sample %in% remove$Sample))

# write data to file
write_csv(final.LD.data, "../../data/rarefaction_LD_data.csv")

```

```{r}

# need to also create a csv for the rarefaction and LD analysis in which SNPs with greater than 10% missing data and greater than 95% allelic frequency have been removed (tomato_snps_first_cull). Also need to remove the duplicated accessions from this rarefaction/LD dataset. Start by transposing tomato_snps_first_cull

write_tsv(tomato_snps_first_cull, "temp.tsv")
tran_tomato_snps_first_cull <- as.tibble(transpose_headed_file("temp.tsv", read_tsv))
unlink("temp.tsv")

# use list of good accessions to remove the appropriate duplicate accessions from the rarefaction/LD dataset. Then write the rarefaction/LD dataset as a csv

rarefaction_LD_data <- anti_join(tran_tomato_snps_first_cull, remove_accessions, by = c("Sample" = "Sample"))
write_csv(rarefaction_LD_data, "../../data/rarefaction_LD_data.csv")

```


ONE LAST THING-- make SNP dataset for PC analysis; I'll do that tomorrow.
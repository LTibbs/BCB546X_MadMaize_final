---
title: "FinalREADME"
author: "Ben Cortes"
date: "December 3, 2018"
output: html_document
---

```{r}
library(tidyverse)
library(readxl)
```

Transpose function that works on tables with headers. First imports table with headers as part of the table, then transposes, then removes the top row and makes it the new headers. Transpose SNP source file so that SNPS are now the rows

```{r}

# transpose function for files with headers, second argument allows specifying what reading function to use (read_csv/tsv/etc)

transpose_headed_file <- function(fname, read_type) {

  # set fun as second argument (which is a reading function). Use fun to read in the first argument (file) ignoring the first row (headers)

  fun = read_type
  tempfile <- fun((paste(fname)), col_names = F)

  # transpose tempfile, then coerce it into a tibble. Setfirst row as column headers, then delete first row

  new_tib <- as.tibble(t(tempfile))
  names(new_tib) <- as.character(unlist(new_tib[1,]))
  new_tib <- new_tib[-1,]
  return(new_tib)
}

tran_tomatosnps <- transpose_headed_file("../../data/Suppl_Table_2.csv", read_csv)

```

SNP source file lacked a header for column 1, so gave it one for downstream purposes. Mutate to add a new column called NA_count. Sum up the number of NAs per row, and set that value as NA_count for that row

```{r}

colnames(tran_tomatosnps)[1] <- "X1"
tran_tomatosnps <- mutate(tran_tomatosnps, NA_count = rowSums(is.na(tran_tomatosnps)))

```

Remove SNPs with greater than 10% missing data (more than 10% of row is NA). Authors removed 240, reducing the original total from 7720 to 7480. Remove NA count column afterwards. The following code reduces the original SNP count down to 7469.

```{r}

tomato_snps_no_missing.L <- tran_tomatosnps %>%
  filter(NA_count <= (0.1 * (ncol(tran_tomatosnps) - 2)))
tomato_snps_no_missing.L <- tomato_snps_no_missing.L[,-1010]

```

Remove SNPs where major allele frequency >0.95. Authors removed 1137 SNPs, this code removes 992 SNPs. 

```{r}

# find major allele (aka reference allele here) for each SNP
num.geno <- tomato_snps_no_missing.L %>%
  unite(united, -1, sep="", remove=F) %>%
  mutate(united_noNA=str_replace_all(united, "NA", ""), # remove NA values
         count.A=str_count(united_noNA, "A"), # count each nucleotide in the SNPs
         count.C=str_count(united_noNA,"C"),
         count.G=str_count(united_noNA, "G"),
         count.T=str_count(united_noNA,"T"),
         max.count=pmax(count.A, count.C, count.G, count.T), # find the max count of any NT for a given SNP
         ref.allele=ifelse(count.A==max.count, "A", # set the reference allele
                           ifelse(count.C==max.count, "C",
                                  ifelse(count.G==max.count, "G", "T")))
  ) 

# find major allele frequency
num.geno <- num.geno %>%
  mutate(major.af=max.count/str_length(united_noNA)) # major allele frequency is the count of the major allele (max.count) / total # of alleles known for that SNP (the length of the string united_noNA)

# remove snps with major allele freq > 0.95 (that is, keep those with minor allele freq <= 0.95); remove unneeded columns and rename others
tomato_snps_first_cull.L <- num.geno %>%
  filter(major.af<=0.95) %>%
  select(-united_noNA, -united, -count.A, -count.C, -count.G, -count.T, -max.count, -ref.allele, -major.af) %>%
  rename(marker=X1)

```

Remove SNPs that are separated by less than 0.1 centi-Morgans of genetic distance. Typically, when studies remove SNPs based on genetic distance they remove only one of the two SNPs that fall below the distance cutoff. The authors do not state how they chose which SNP to remove (whether they chose the SNP with the lower distance, the higher distance, or if they chose randomly). According to the paper, "genetic distances were based on the genetic maps of Sim et al." (https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0040563)." However, that reference has eight different genetic maps, and the authors don't state which one(s) they used. After reading through the paper, Table S8 seems to be the most complete genetic map, so this is the primary one used in the code. This map was generated by Sim et al. from two preexisting maps of the tomato crosses EXPEN and EXPIM. The first rows contain SNPs found in both the EXPEN and EXPIM maps, and the later rows are those SNPS that are found in only one or the other. 

In order to use the genetic map (Table S8) to remove SNPs that fall within 0.1 cM the SNPs common between the EXPEN and EXPIM maps that compose Table S8 to create a consensus map. The paper does not mention how this was calculated, so we used the package LPmerge (Jeffrey Endelman, "Merging Linkage Maps by Linear Programming, 2018). The following code generates the input files for LPmerge, one input file for each chromosome.

```{r}

# need to have a separate input file for each map, for each chromosome--so, need to split the files by map and chromosome
# each file should be formatted to have headers "marker" and "cM" and the observations for these variables in the rows beneath

# read raw map file and tidy it
S8_map.L <- read_csv("../../data/S8_Map_Sim_etal.csv")
# set column names based on inspection of original csv:
colnames(S8_map.L) <- c("Group", "marker", "physical.chr", "physical.mbp", "EXPEN.chr", "EXPEN.cM", "EXPIM.chr", "EXPIM.cM")
# remove rows that contain column names rather than data
S8_map.L <- S8_map.L[-c(1:2),]
# correct type of columns:
S8_map.L$EXPEN.cM <- as.numeric(S8_map.L$EXPEN.cM)
S8_map.L$EXPIM.cM <- as.numeric(S8_map.L$EXPIM.cM)

# visually check:
head(S8_map.L)

# separate into EXPEN and EXPIM maps:
# check the values in the different columns to potentially use to split the file
unique(S8_map.L$Group) 
unique(S8_map.L$EXPEN.chr)
unique(S8_map.L$EXPIM.chr)
# EXPIM.chr contains not only chromosomes listed as 1,2,3,... but also as CHR01, CHR02, ...
# standardize these chromosome names
S8_map.L$EXPIM.chr <- gsub("CHR0", "",S8_map.L$EXPIM.chr)
S8_map.L$EXPIM.chr <- gsub("CHR", "", S8_map.L$EXPIM.chr)
# and check results:
unique(S8_map.L$EXPIM.chr)
# and make all chromosomes numeric:
S8_map.L$EXPIM.chr <- as.numeric(S8_map.L$EXPIM.chr)
S8_map.L$EXPEN.chr <- as.numeric(S8_map.L$EXPEN.chr)

# separate into maps for EXPEN and EXPIM, separated by chromosome
# process the EXPIM map
map.EXPIM <- filter(S8_map.L, Group %in% c("Common", "EXPIM 2012")) %>% #separate out SNPs in EXPIM
  select(-c(Group, physical.chr, physical.mbp, EXPEN.chr, EXPEN.cM)) %>% # remove unneeded columns
  rename(chr=EXPIM.chr, cM=EXPIM.cM) %>% # rename to match LPmerge format
  group_by(chr) #group by Group
split.EXPIM <- split(map.EXPIM, map.EXPIM$chr) # split dataframes by chr and save in a list
# sanity check: no missing chromosomes
stopifnot(sum(is.na(map.EXPIM$chr))==0)
# remove chromosome column
for (i in 1:length(split.EXPIM)) {
  split.EXPIM[[i]] <- split.EXPIM[[i]]%>%
    ungroup()%>%
    select(-chr)
}

# repeat for EXPEN
map.EXPEN <- filter(S8_map.L, Group %in% c("Common", "EXPEN 2012")) %>% #separate out SNPs in EXPEN
  select(-c(Group, physical.chr, physical.mbp, EXPIM.chr, EXPIM.cM)) %>% # remove unneeded columns
  rename(chr=EXPEN.chr, cM=EXPEN.cM) %>% # rename to match LPmerge format
  group_by(chr) #group by Group
split.EXPEN <- split(map.EXPEN, map.EXPEN$chr) # split dataframes by chr and save in a list
# sanity check: no missing chromosomes
stopifnot(sum(is.na(map.EXPEN$chr))==0)
# remove chromosome column
for (i in 1:length(split.EXPEN)) {
  split.EXPEN[[i]] <- split.EXPEN[[i]]%>%
    ungroup()%>%
    select(-chr)
}

```

LPmerge creates multiple consensus maps and outputs them as dataframes in a list along with statistics for each dataframe. The user must then manually assess the quality of each dataframe via the statistics and pick the best one (the one with "the lowest mean and sd for RMSE"). Lowest mean value was prioritized.

```{r}
# LPmerge run and create consensus map

library(LPmerge)

# Make a function to run LPMerge:
# based on Ben's function to run LPmerge, but using the df already loaded rather than writing and then reading in files:
map_merger.L <- function(EXPEN.df, EXPIM.df) {
  
  # make into data frames:
  EXPEN.df <- as.data.frame(EXPEN.df)
  EXPIM.df <- as.data.frame(EXPIM.df)
  
  # put in list, then format as string (from LPMerge tutorial at https://potatobreeding.cals.wisc.edu/wp-content/uploads/sites/161/2014/01/LPmerge_tutorial.pdf)
  Maps <- list(EXPEN.df, EXPIM.df)
  names(Maps) <- c("EXPEN", "EXPIM")
  str(Maps)

  # run LPmerge on these files, using default settings for max.interval:
  merged.map <- LPmerge(Maps, max.interval = 1:4)
  
}

# sanity check--are both input lists the same size?
stopifnot(length(split.EXPEN)==length(split.EXPIM))

# Use the function above to run LPmerge on all chromosomes
merged_maps.L <- list()

for (i in 1:length(split.EXPEN)) {
  merged_maps.L[[i]] <- map_merger.L(split.EXPEN[[i]], split.EXPIM[[i]])
}

# manually inspect the output to determine which of the four consensus maps generated for each chromosome is the best.
# the criteria, according to the LPmerge tutorial (at https://potatobreeding.cals.wisc.edu/wp-content/uploads/sites/161/2014/01/LPmerge_tutorial.pdf): choose the lowest mean RMSE, and break ties using the lowest sd for RMSE.

# After doing so, the results for the best consensus maps are, for chromosomes 1-12 in order:
best <- c(1,2,4,1,1,1,3,4,3,3,1,2)

# Now, make the overall consensus map:

# as a list:
consensus.map.list <- list()
for (i in 1:length(merged_maps.L)) {
  consensus.map.list[[i]] <- merged_maps.L[[i]][[best[i]]] %>%
    mutate(chr=i) %>% # add column to save chromosome number (I visually inspected to be certain that merged_maps.L[[i]] does in fact correspond to chromosome i)
    select(marker, chr, cM=consensus) # arrange (and rename) needed columns
}

# as one dataframe:
consensus.map <- as.tibble(do.call("rbind", consensus.map.list))

```

However, the SNPs in the consensus map don't correspond perfectly with the SNPs in the dataset. There are 6471 SNPs in my data but only 5296 SNPs in the consensus map. These discrepancies arose from the fact that different names were used for the SNPs in the author's paper and in the SIm paper. The following code rectifies the discrepancies between those SNPs in the dataset and those in the consensus map. It then removes all SNPs separated by less than 0.1 cM. Authors state that 4030 SNPs were removed, but this code removes 3510.

```{r}

# compare SNPs in consensus map with those in our dataset
map.markers <- S8_map.L$marker
my.markers <- tomato_snps_first_cull.L$marker

shared <- map.markers[map.markers %in% my.markers]

map.only <- map.markers[!(map.markers %in% my.markers)]
my.only <- my.markers[!(my.markers %in% map.markers)]
# there are 4206 shared snps + 1090 only in map = 5296 in map. OK.
# there are 4206 shared snps + 2265 only in mine = 6471 in my markers. OK.

# However, looking at the Sim et al. paper, their Table S1 contains alternative names for all the SNPs, which we have downloaded and placed in the `data` folder as `S1_SNPs_Sim_etal.xlsx`. Are these the names that were used in our paper, and could this increase the number of shared SNPs? Check for this.

# read in file and tidy:
sim.snps <- read_xlsx("../../data/S1_SNPs_Sim_etal.xlsx")
# based on manual inspection of original file, column names have been moved to the first row. Fix this.
colnames(sim.snps) <- sim.snps[1,]
sim.snps <- sim.snps[-1,]
# select and rename needed columns
sim.snps <- sim.snps %>%
  select(SNP=`SNP Name`, Alt.SNP=`SolCAP SNP ID`)

# check if these other names agree
test <- left_join(consensus.map, sim.snps, by=c("marker"="SNP")) %>%
  rename(marker.map=marker) # rename marker to know which df this "marker" came from
stopifnot(sum(is.na(test$chr))==0)
test2 <- left_join(test, tomato_snps_first_cull.L, by=c("Alt.SNP"="marker"))
stopifnot(sum(is.na(test2$chr))==0)

# This worked! All 5296 SNPs in the consensus map can now be matched to those in the Blanca paper data. However, there are still more SNPs in the Blanca data than in the consensus map. Checking the original paper from which the consensus maps came, we are in fact using the most complete map; these other SNPs that are in our dataset simply are not mapped. 

# make new consensus map with the SNPs named the same as in Blanca et al:
new.consensus.map <- test2 %>%
  select(marker=Alt.SNP, chr, cM) #extract and rename relevant columns

# remove unneeded files
rm(test, test2)

# Now, thin these based on the 0.1 cM criteria:
# The text of the methods of the Blanca, et al. paper said that "SNPs that mapped closer than 0.1 cM were removed". So, I will proceed under the assumption that the SNPs that ARE found in the map were filtered following this criteria, but that then the SNPs that were not in the map were simply retained, because they cannot be KNOWN to map closer than 0.1 cM.
# Also, the paper did not say how they decided which SNP out of a group of nearby SNPs to remove, so we will arbitrarily choose to keep the first SNP found and then exclude all SNPs found within the next 0.1 cM, then repeat the process from the next SNP that is outside that region. 

# find distance between SNPs and keep if they are >=0.1 cM from one another
consensus.map.thinned <- new.consensus.map %>%
  mutate(distance.lag = cM-lag(cM), # find distance between adjacent SNPs
         KEEP=ifelse(chr != lag(chr),TRUE,ifelse(distance.lag >= 0.1,TRUE,FALSE))) # mark to keep SNPs that are separated by at least 0.1 cM, as well as those on the end of the chromosome

# check that this worked before filtering:
consensus.map.thinned %>% filter(is.na(KEEP)) # the only entry with NA is the first one, which is ok; keep it

# It is potentially possible that there may be many SNPs in a row that are each within 0.1 cM of one another and so none of them will be kept, but that span a total distance of over 0.1 cM and therefore at least some of them should be kept. For example, if there were SNPs on chr 1 at 0, 0.05, 0.12, and 0.3 cM, the algorithm above would keep only 0 and 0.3 when in reality we want to keep 0, 0.12, and 0.3 so that each SNP that is kept is separated by more than or equal to 0.1 cM. These cases will only be found among SNPs that were marked KEEP==FALSE and which are separated from the nearest SNP that is also marked KEEP==FALSE by less than 0.1 cM (as explained above), and more than 0 cM (otherwise they are identical and only one should in fact be kept).  Below, use the column "inspect" to indicate a member of a potential group of SNPs that could be causing this problem. 
check <- consensus.map.thinned %>%
  filter(KEEP == F) %>%
  filter(distance.lag>0) %>%
  mutate(new.distance.lag = cM-lag(cM),
         inspect= ifelse((new.distance.lag<=0.1 & new.distance.lag >0),"YES","-")) 
sum(check$inspect=="YES", na.rm=T)
# There are only 17 cases where this problem could potentially occur. Although a larger data set could make this solution intractable, for this small dataset I will manually inspect the data to find if any of these SNPs should in fact be included.

# After manual inspection, this situation did occur 5 times. To solve, need to manually include the following SNPs:
# solcap_snp_sl_25987
# solcap_snp_sl_100795
# solcap_snp_sl_25442
# solcap_snp_sl_43223
# solcap_snp_sl_19031
# also want to manually keep the first SNP (currently, KEEP==NA for that snp): solcap_snp_sl_15058
consensus.map.thinned$KEEP[consensus.map.thinned$marker=="solcap_snp_sl_25987"] <- TRUE
consensus.map.thinned$KEEP[consensus.map.thinned$marker=="solcap_snp_sl_100795"] <- TRUE
consensus.map.thinned$KEEP[consensus.map.thinned$marker=="solcap_snp_sl_25442"] <- TRUE
consensus.map.thinned$KEEP[consensus.map.thinned$marker=="solcap_snp_sl_43223"] <- TRUE
consensus.map.thinned$KEEP[consensus.map.thinned$marker=="solcap_snp_sl_19031"] <- TRUE
consensus.map.thinned$KEEP[consensus.map.thinned$marker=="solcap_snp_sl_15058"] <- TRUE

# Now, can pull out the list of SNPs to remove:
remove.snps <- consensus.map.thinned$marker[consensus.map.thinned$KEEP==F]

# and remove these snps from the dataset created above after MAF and missing data processing:
final.snps <- tomato_snps_first_cull.L %>%
  filter(!(marker %in% remove.snps))

```

Go through and remove duplicate accessions (lines of tomatoes that were genotyped twice) from the text file 12864_2015_1444_MOESM1_ESM containing the passport data. The names of the accessions in this file differ from their names in the raw SNP datafile, so the following code was used to change the names in the passport file to those of the SNP file.

```{r}
# process data before removing duplicate accessions

# transpose final.snps so that it matches the original layout of the genotype file
tran.final.snps <- as_tibble(cbind(Sample=colnames(final.snps), t(final.snps))) # transpose final.snps and cbind the accession names to it
colnames(tran.final.snps) <- c("Sample", tran.final.snps[1,-1]) # add the column names since they were moved to row 1 by t()
tran.final.snps <- tran.final.snps[-1,] # remove row 1 now that column names are where they should be

# remove row of NA_count
head(tran.final.snps) # visually inspect transposed geno file

# import accessions' passport information, and edit names to match those in the snps file (following code found in `2_Eigensoft_geno_format.Rmd`)
passport.data <- read_tsv("../../data/12864_2015_1444_MOESM1_ESM.txt")
# After manual inspection, some of the problems seem to be: 
# different cases (upper vs lower),
# periods vs commas, 
# and spaces vs underscores
# So, manually edit passport data so that it will match SNP data:
passport.data$Sample <- gsub("\\,", "\\.", passport.data$Sample ) # switch commas to periods
passport.data$Sample  <- gsub("PAS16398", "pas16398", passport.data$Sample ) # change case
passport.data$Sample  <- gsub("PAS16401", "pas16401", passport.data$Sample ) # change case
passport.data$Sample  <- gsub("PI 379051", "PI_379051", passport.data$Sample ) # change space to underscore
# check that this fixed problem and all entries match up:
stopifnot(sum(!(passport.data$Sample %in% tran.final.snps$Sample))==0)
stopifnot(sum(!(tran.final.snps$Sample %in% passport.data$Sample))==0)

```

According to the authors: 

"When an accession was genotyped more than once and both genotypes were inconsistent (e.g., both samples were classified in different subgroups in the PCA) all data for the accession was removed from the analysis (see Additional file 1: Table S1), unless it was clear based on the passport information, which genotype was correct (e.g., two entries from the same SLC accession collected in Peru, one grouping with other Peruvian accessions and another grouping with the mixture group). In total 8 genotypes out of the 1,008 were removed due to inconsistent data...unless stated to the contrary, only one randomly chosen genotype representative of the uniquely named accessions was used."

What exactly "unless it was clear based on the passport information" means could vary from the provided example depending on the situation, so was not amenable to coding. We decided that the best thing to do would be to go through and inspect the duplicate data manually after arranging it. The following workflow was used in Excel.

1. The file used (`duplicate_accessions.csv`) was generated in R (below) by filtering for duplicated accessions in the passport.data tibble.
2. The accession duplicates were checked for inconsistencies in the columns that contain information from the PCA; that is, the species, group1, and group2 columns.
3. If there were no inconsistencies found, one sample was chosen at random (using a random number generator) for that accession to keep. The other samples for the accession were marked to delete.
4. If inconsistencies were found, the passport and geographic data columns were assessed to determine which accession was "correct." We took "conflicting classification" to indicate that a given sample was not the correct sample for that accession. However, if it was not clear which sample was correct, we removed the accession entirely.
5. We saved the file with the lines to delete and keep marked as `duplicate_accessions_new.csv`.

Once duplicate accessions were removed, write the final dataset to a .csv.

```{r}
# export passport file to use to inspect duplicates in Excel

# find duplicate accessions in the passport file:
dups <- passport.data$Accession[duplicated(passport.data$Accession)] 
u.dups <- unique(dups)
passport.data.dup <- passport.data %>%
  filter(Accession %in% u.dups) %>%
  arrange(Accession)
write_csv(passport.data.dup, "duplicate_accessions.csv")

# after the above workflow in Excel, read the data in:
new.passport.data.dup <- read_csv("duplicate_accessions_new.csv") 

remove <- filter(new.passport.data.dup, `keep? (1 for yes, 0 for no)`==0) %>%
  select(Sample)
remove <- remove[[1]]

# However, at this point, also noticed in the original data file that there is another individual with "conflicting classification" which is not a duplicate, but should also be removed: LA2076. Add this to remove.
remove <- c(remove, "LA2076")

# remove the samples to be removed from the dataset tran.final.snps:
final.data <- filter(tran.final.snps, !(Sample %in% remove))

test <- left_join(final.data, new.passport.data.dup, by="Sample") %>%
  filter(!(`keep? (1 for yes, 0 for no)`==0)) %>%
  select(Sample, group2)

# write the final dataset:
write_csv(final.data, "../../data/final_data.csv")
```

For the rarefaction and LD analysis, however, a different dataset was used where SNPs were not filtered out if they fell within 0.1 cM of each other. SNPs with greater than 10% missing data and greater than 0.95 major allelic frequency were still removed, and we assumed that duplicate accessions were removed as above. 

```{r}

# transpose `tomato_snps_first_cull.L`
LD.data <- as_tibble(cbind(Sample=colnames(tomato_snps_first_cull.L), t(tomato_snps_first_cull.L))) # transpose tomato_snps_first_cull.L and cbind the accession names to it
colnames(LD.data) <- c("Sample", LD.data[1,-1]) # add the column names since they were moved to row 1 by t()
LD.data <- LD.data[-1,] # remove row 1 now that column names are where they should be
head(LD.data) # visually inspect transposed geno file

# remove duplicates
final.LD.data <- filter(LD.data, !(Sample %in% remove))

# write data to file
write_csv(final.LD.data, "../../data/rarefaction_LD_data.csv")

```
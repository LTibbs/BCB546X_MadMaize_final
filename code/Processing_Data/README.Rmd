---
title: "README"
author: "Ben Cortes"
date: "November 16, 2018"
output: html_document
---

```{r}
library(tidyverse)
```

Transpose function that works on tables with headers. First imports table with headers as part of the table, then transposes, then removes the top row and makes it the new headers. Transpose SNP source file (SNPs as rows)

```{r}

# transpose function for files with headers, second argument allows specifying what reading function to use (read_csv/tsv/etc)

transpose_headed_file <- function(fname, read_type) {                      
  
  # set fun as second argument (which is a reading function). Use fun to read in the first argument (file) ignoring the first row (headers)
  
  fun = read_type 
  tempfile <- fun((paste(fname)), col_names = F)            
  
  # transpose tempfile, then coerce it into a tibble. Setfirst row as column headers, then delete first row
  
  new_tib <- as.tibble(t(tempfile))                             
  names(new_tib) <- as.character(unlist(new_tib[1,]))            
  new_tib <- new_tib[-1,]                                        
  return(new_tib)                                                   
}

tran_tomatosnps <- transpose_headed_file("../../data/Suppl_Table_2.csv", read_csv)       

```

SNP source file lacked a header for column 1, so gave it one for downstream purposes. Mutate to add a new column called NA_count. Sum up the number of NAs per row, and set that value as NA_count for that row

```{r}

colnames(tran_tomatosnps)[1] <- "Sample" 
tran_tomatosnps <- mutate(tran_tomatosnps, NA_count = rowSums(is.na(tran_tomatosnps)))
write_tsv(tran_tomatosnps, "output_tests/tran_tomatosnps.tsv")

```

Remove SNPs with greater than 10% missing data (more than 10% of row is NA). Authors removed 240, reducing the original total from 7720 to 7480. For some reason the following code reduces the original SNP count down to 7469.

```{r}

tomato_snps_no_missing <- tran_tomatosnps %>%
  filter(NA_count <= (0.1 * (ncol(tran_tomatosnps) - 2)))
write_tsv(tomato_snps_no_missing, "output_tests/tomatosnps_nomissing.tsv")

```

Remove SNPs where major allele frequency >0.95. Authors removed 1137 SNPs. 

```{r}

# First, unite contents of all rows except for the Sample column into a string

good_allele_freq <- tomato_snps_no_missing %>%
  unite("string_format", -"Sample", sep = "")

# Next, make a new column with the row strings, but with all "NA"s removed. Unite will take NA values and insert them into the string as "NA." Create columns to count the total alleles (and to check for any leftover "N"s from "NA"s). Remove original string column. 

good_allele_freq <- mutate(good_allele_freq, true_string = str_replace_all(string_format, "NA", "")) %>%
  mutate(a_count = str_count(true_string, "A"),
                 g_count = str_count(true_string, "G"),
                 c_count = str_count(true_string, "C"),
                 t_count = str_count(true_string, "T"),
                 n_count = str_count(true_string, "N"),
                 string_format = NULL)

# Finally, filter out any row that has a base_count value higher than 95% of the lenght of the true_string string for that row. The following code reduces the total SNPs to 6477, but the authors' code reduced it to 6343 (134 less).

good_allele_freq <- filter(good_allele_freq, a_count <= (0.95 * (str_length(true_string)))) %>%
              filter(g_count <= (0.95 * (str_length(true_string)))) %>%
              filter(c_count <= (0.95 * (str_length(true_string)))) %>% 
              filter(t_count <= (0.95 * (str_length(true_string)))) 

# Now join the SNPs that remain after removing the SNPs with higher than 95% allele frequency to the tibble of SNPs that have less than 10% missing data

tomato_snps_first_cull <- left_join(good_allele_freq, tomato_snps_no_missing, by = c("Sample" = "Sample"))
tomato_snps_first_cull <- tomato_snps_first_cull[, -c(2:7,1016)]
write_tsv(tomato_snps_first_cull, "output_tests/tomatosnps_first_cull.tsv")

```

Remove SNPs that are separated by less than 0.1 centi-Morgans of genetic distance. According to the rest of my group (who works with plants and plant genomics), when studies remove SNPs based on genetic distance they remove only one of the two SNPs that fall below the distance cutoff. The authors do not state how they chose which SNP to remove (whether they chose the SNP with the lower distance, the higher distance, or if they chose randomly). According to the paper, "genetic distances were based on the genetic maps of Sim et al." (https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0040563)." However, that reference has eight different genetic maps, and the authors don't state which one(s) they used. After reading through the paper, Table S8 seems to be the most complete genetic map, so I'm using that one. This map was generated by Sim et al. from two preexisting maps of the tomato crosses EXPEN and EXPIM. The first rows contain SNPs found in both the EXPEN and EXPIM maps, and the later rows are those SNPS that are found in only one or the other. 

Separate Table S8 into 3 tibbles, each with the SNP column. PhysMap contains the physical locations of each SNP in Mbp. EXPEM and EXPIM contain the location of each SNP in cM; these later two have missing values for SNPS not found in their respective maps.

```{r}

# Read in the csv, then remove top two rows and give new column headers to get around the merged cells in the original csv file that disrupt the tibble. Separate into EXPEN and EXPIM files

S8_map <- as.tibble(read_csv("../../data/S8_Map_Sim_etal.csv"))
S8_map <- S8_map[-c(1,2),]
colnames(S8_map) <- c("Group", "SNP_ID", "Physmap_Chr", "Physmap_Mbp", "EXPEN_Chr", "EXPEN_cM", "EXPIM_Chr", "EXPIM_cM")

EXPEN <- S8_map[, c(2,5,6)]
EXPIM <- S8_map[, c(2,7,8)]

# EXPIM map had some chromosomes listed as "CHR#" instead of just "#". Used gsub to replace

EXPIM$"EXPIM_Chr" <- gsub("CHR0", "", EXPIM$"EXPIM_Chr")
EXPIM$"EXPIM_Chr" <- gsub("CHR", "", EXPIM$"EXPIM_Chr")

# Arrange by chromosome column, then by cM. Remove loci that aren't polymorphic in that map (NA)

EXPEN <- arrange(EXPEN, as.numeric(EXPEN_Chr), as.numeric(EXPEN_cM)) 
EXPIM <- arrange(EXPIM, as.numeric(EXPIM_Chr), as.numeric(EXPIM_cM))

EXPEN <- EXPEN[complete.cases(EXPEN),]
EXPIM <- EXPIM[complete.cases(EXPIM),]

```

Using function created for the R assignment, split rows of EXPEN and EXPIM into smaller files based on chromosome. Map joining package will need separated files. 

Files with identical values in a user-defined, ordered column will be added to same file (ie, all files with same value in a column will be placed into a file). Output to files with filenames bearing the pattern "'third input' 'value of column at given iteration' 'fourth input'". Input: dataframe, number of column, anything to be written in the output filename before the column value (including the file path), anything to be written in the output filename after the column value (including the file type).

```{r}

split_by_ordered_column <- function(fname, cnum, prefix, suffix) {
#dataframe to split = fname
#column to split by = cnum
#first part of filename = prefix
#second part of filename = suffix

for (i in 1:nrow(fname)) {
#iterate through all rows of dataframe
  
  if(i == 1 || fname[i,cnum] != fname[i-1,cnum]) {
  #if the first row, or if the column value isn't the same as the previous one
      
    write_tsv(fname[i,], paste(prefix, fname[i,cnum], suffix, sep = ""))
    #create a dataframe named with the current (ith) row named("prefix, current column
    #value, suffix")  
    
    } else {
        write_tsv(fname[i,], paste(prefix, fname[i,cnum], suffix, sep = ""), append = T)
    #otherwise, append ith row to current dataframe
      
    }
  }
}

split_by_ordered_column(EXPEN, 2, "sorted_data/CHR_", "_EXPEN")
split_by_ordered_column(EXPIM, 2, "sorted_data/CHR_", "_EXPIM")

```

In order to use the genetic map (Table S8) to remove SNPs that fall within 0.1 cM the SNPs common between the EXPEN and EXPIM maps that compose Table S8 to create a consensus map. The paper does not mention how this was calculated, so we used the package LPmerge (Jeffrey Endelman, "Merging Linkage Maps by Linear Programming, 2018). LPmerge creates multiple consensus maps and outputs them as dataframes in a list along with statistics for each dataframe. The user must then manually assess the quality of each dataframe via the statistics and pick the best one (the one with "the lowest mean and sd for RMSE"). Lowest mean value was prioritized.

```{r}

library(LPmerge)

map_merger <- function(file1, file2) {
# call library and create function to run LPmerge on two files
  
  file1_merge <- as.data.frame(read_tsv(file1))
  file2_merge <- as.data.frame(read_tsv(file2))
  # read in the files as dataframes (LPmerge needs dataframes)
  
  map2 <- list(file1_merge[,c(1,3)], file2_merge[,c(1,3)])
  names(map2) <- c("EXPEN", "EXPIM")
  str(map2)
  # add the SNP_ID and cM columns to a list, give the list names, and format it as a string
  
  merged_map2 <- LPmerge(map2, max.interval=1:4)
  # call LPmerge which outputs a list with four datafables
  
}

# add all chromosome files for EXPEN and EXPIM to vectors

EXPEN_list <- c("sorted_data/CHR_1_EXPEN", "sorted_data/CHR_2_EXPEN", "sorted_data/CHR_3_EXPEN", "sorted_data/CHR_4_EXPEN", "sorted_data/CHR_5_EXPEN", "sorted_data/CHR_6_EXPEN", "sorted_data/CHR_7_EXPEN", "sorted_data/CHR_8_EXPEN", "sorted_data/CHR_9_EXPEN", "sorted_data/CHR_10_EXPEN", "sorted_data/CHR_11_EXPEN", "sorted_data/CHR_12_EXPEN")

EXPIM_list <- c("sorted_data/CHR_1_EXPIM", "sorted_data/CHR_2_EXPIM", "sorted_data/CHR_3_EXPIM", "sorted_data/CHR_4_EXPIM", "sorted_data/CHR_5_EXPIM", "sorted_data/CHR_6_EXPIM", "sorted_data/CHR_7_EXPIM", "sorted_data/CHR_8_EXPIM", "sorted_data/CHR_9_EXPIM", "sorted_data/CHR_10_EXPIM", "sorted_data/CHR_11_EXPIM", "sorted_data/CHR_12_EXPIM")

# iterate through the vectors of chromosome files and run the merging function on them

merged_maps <- list()

for (i in 1:12) {
  merged_maps[[i]] <- map_merger(EXPEN_list[i], EXPIM_list[i])
}

```

Manually sort through statistics for each of the four datafames per chromosome, and pick the one with the lowest mean (use lowest standard deviation for ties). Then merge the dataframes into one and calculate the genetic distance between each SNP. Finally, remove SNPs which are greater than 0.1 cM separated from their neighbors.

```{r}

# use rbind to bind the rows of the 12 dataframes into one. Then remove all SNPs from millimorgan_snps that aren't present in the authors' original dataset

consensus_map <- rbind(merged_maps[[1]][[1]], merged_maps[[2]][[2]], merged_maps[[3]][[4]], merged_maps[[4]][[1]], merged_maps[[5]][[1]], merged_maps[[6]][[1]], merged_maps[[7]][[2]], merged_maps[[8]][[4]], merged_maps[[9]][[3]], merged_maps[[10]][[3]], merged_maps[[11]][[1]], merged_maps[[12]][[2]])

millimorgan_snps <- consensus_map[,1:2]
millimorgan_snps <- semi_join(millimorgan_snps, tomato_snps_first_cull, by = c("marker" = "Sample"))

# Use mutate to add an additional column, distance, which result of subtracting the previous consensus value from the the consensus value for a given row. Use lag to reference the previous value 

millimorgan_snps <- mutate(millimorgan_snps, distance = consensus - lag(consensus))

# use filter to remove SNPs which are less than 0.1 cM from the previous SNP. In theory, this would be a problem if there were many consecutive SNPs separated by distances of < 0.1 cM such that the total distance spanned from the first to last SNP was actually greater than 0.1 cM (since the filter paramaters only compare two consecutive SNPs). However, a cursory look through the data turns up none of these scenarios. The second two arguments account for the first SNP in chromosome 1 (the first SNP in the dataframe) and the first SNP in the next 11 chromosomes. Authors state that their final number of SNPS was 2313, but the result of this code yields a remaining 1461 SNPs.

millimorgan_snps <- filter(millimorgan_snps, (distance > 0.1 | is.na(distance) | distance < 0))
write_tsv(millimorgan_snps, "output_tests/millimorgansnps.tsv")

```

Create the final SNP file. Join the list of SNPs that are separated by at least 0.1 cM to the list of SNPs that have less than 10% missing data and an allele frequency of less than 95%.  Write output to a tsv.

```{r}

final_SNPs <- semi_join(tomato_snps_first_cull, millimorgan_snps, by = c("Sample" = "marker"))

write_tsv(final_SNPs, "final_snps.tsv")
write_tsv(final_SNPs, "output_tests/final_snps.tsv")

```

Go through and remove duplicate accessions (lines of tomatoes that were genotyped twice) from the text file 12864_2015_1444_MOESM1_ESM containing the passport data. The names of the accessions in this file differ from their names in the raw SNP datafile, so Laura's code was used to change the names in the passport file to those of the SNP file.

```{r}

# transpose the final_SNPs file (which has all pertinent SNPs removed). Make the column name for the new first column, the accession column, "Accession." Arrange by the Accession column

tran_final_snps <- as.tibble(transpose_headed_file("final_snps.tsv", read_tsv))
colnames(tran_final_snps)[1] <- "Accession"
tran_final_snps <- arrange(tran_final_snps, as.character(Accession))
write_tsv(tran_final_snps, "output_tests/tran_final_snps.tsv")

# import passport data and begin changing accession names

passport_data <- as.tibble(read_tsv("../../data/12864_2015_1444_MOESM1_ESM.txt"))

# start Laura's code, variable names modified:
# After manual inspection, some of the problems seem to be: 
# different cases (upper vs lower; lowercase in pre.indiv),
# periods vs commas (periods in pre.indiv), 
# and spaces vs underscores (underscores in pre.indiv)
# So, manually edit full.indiv so that it will match pre.indiv:

passport_data$Sample <- gsub("\\,", "\\.", passport_data$Sample) # switch commas to periods

# Now only 3 are not joined, so manually edit these to match:
passport_data$Sample <- gsub("PAS16398", "pas16398", passport_data$Sample) # change case
passport_data$Sample <- gsub("PAS16401", "pas16401", passport_data$Sample) # change case
passport_data$Sample <- gsub("PI 379051", "PI_379051", passport_data$Sample) # change space to underscore

write_tsv(passport_data, "passport_data.tsv")

```

According to the authors 

"When an accession was genotyped more than once and both genotypes were inconsistent (e.g., both samples were classified in different subgroups in the PCA) all data for the accession was removed from the analysis (see Additional file 1: Table S1), unless it was clear based on the passport information, which genotype was correct (e.g., two entries from the same SLC accession collected in Peru, one grouping with other Peruvian accessions and another grouping with the mixture group). In total 8 genotypes out of the 1,008 were removed due to inconsistent data."

What exactly "unless it was clear based on the passport information" means could vary from the provided example depending on the situation, so we decided that the best thing to do would be to go through the data manually after arranging it. The following workflow was used in Excel.

1. Duplicates in the Accesion column were found by selecting the column, then choosing conditional formatting for duplicates.
2. The accession duplicates were checked for inconsistencies in the group1 and group2 columns.
3. If there were no inconsistencies found, the accession was noted for one of the copies to be deleted at random.
4. If inconsistencies were found, the passport and geographic data columns were assessed to determine which accession was "correct." We took "conflicting classification" to indicate that it was impossible to tell which was "correct."
5. Like the authors, we deleted 8 accessions. The final list of accessions, good_accessions.txt, was then used to remove the unwanted accessions from the data.

```{r}

# import list of accessions that are to be removed. Antijoin the list of accessions to be removed with the transposed final SNPs file, resulting in a tibble where all pertinent SNPs have been removed and all the appropriate, duplicated accessions have been removed.

remove_accessions <- as.tibble(read_tsv("removed_accessions.tsv"))
final_data <- anti_join(tran_final_snps, remove_accessions, by = c("Accession" = "Sample"))
write_csv(final_data, "../../data/final_data.csv")

```


```{r}

# need to also create a csv for the rarefaction and LD analysis in which SNPs with greater than 10% missing data and greater than 95% allelic frequency have been removed (tomato_snps_first_cull). Also need to remove the duplicated accessions from this rarefaction/LD dataset. Start by transposing tomato_snps_first_cull

write_tsv(tomato_snps_first_cull, "temp.tsv")
tran_tomato_snps_first_cull <- as.tibble(transpose_headed_file("temp.tsv", read_tsv))
unlink("temp.tsv")

# use list of good accessions to remove the appropriate duplicate accessions from the rarefaction/LD dataset. Then write the rarefaction/LD dataset as a csv

rarefaction_LD_data <- anti_join(tran_tomato_snps_first_cull, remove_accessions, by = c("Sample" = "Sample"))
write_csv(rarefaction_LD_data, "../../data/rarefaction_LD_data.csv")

```


